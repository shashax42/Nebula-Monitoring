# CloudWatch Alarms ê°€ì´ë“œ

## ì•ŒëŒ êµ¬ì„±

### Application Performance (SLO ê¸°ë°˜)

| ì•ŒëŒ | ì„ê³„ê°’ | ì„¤ëª… |
|------|--------|------|
| **High Error Rate** | > 5% | ì—ëŸ¬ìœ¨ì´ 5% ì´ˆê³¼ |
| **High Latency P95** | > 1ì´ˆ | P95 ë ˆì´í„´ì‹œ 1ì´ˆ ì´ˆê³¼ |
| **Low Availability** | < 99.9% | ê°€ìš©ì„± 99.9% ë¯¸ë‹¬ (SLO ìœ„ë°˜) |

### Infrastructure

| ì•ŒëŒ | ì„ê³„ê°’ | ì„¤ëª… |
|------|--------|------|
| **EKS Node CPU** | > 80% | ë…¸ë“œ CPU ì‚¬ìš©ë¥  80% ì´ˆê³¼ |
| **EKS Node Memory** | > 80% | ë…¸ë“œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  80% ì´ˆê³¼ |
| **Pod Restart Rate** | > 5/ë¶„ | Pod ì¬ì‹œì‘ ë¹ˆë„ ê³¼ë‹¤ |

### OTEL Collector Health

| ì•ŒëŒ | ì„ê³„ê°’ | ì„¤ëª… |
|------|--------|------|
| **Collector Down** | ë©”íŠ¸ë¦­ ì—†ìŒ | OTEL Collector ë‹¤ìš´ |
| **Collector Memory** | > 80% | Collector ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³¼ë‹¤ |

### Composite Alarm

| ì•ŒëŒ | ì¡°ê±´ | ì„¤ëª… |
|------|------|------|
| **Service Degradation** | ë³µí•© ì¡°ê±´ | ì—¬ëŸ¬ ì§€í‘œê°€ ë™ì‹œì— ì•…í™” |

## ì•Œë¦¼ ì„¤ì •

### 1. ì´ë©”ì¼ ì•Œë¦¼ ì„¤ì •

```hcl
# terraform/environments/dev/main.tf
module "cloudwatch_alarms" {
  email_endpoints = [
    "ops-team@company.com",
    "on-call@company.com"
  ]
}
```

ë°°í¬ í›„ ì´ë©”ì¼ í™•ì¸ í•„ìš”:
1. AWS SNSì—ì„œ í™•ì¸ ì´ë©”ì¼ ë°œì†¡
2. ì´ë©”ì¼ì˜ "Confirm subscription" í´ë¦­
3. ì•Œë¦¼ ìˆ˜ì‹  ì‹œì‘

### 2. Slack ì•Œë¦¼ ì„¤ì • (Lambda í•„ìš”)

```python
# Lambda í•¨ìˆ˜ ì˜ˆì‹œ
import json
import urllib3

http = urllib3.PoolManager()

def lambda_handler(event, context):
    url = "YOUR_SLACK_WEBHOOK_URL"
    msg = json.loads(event['Records'][0]['Sns']['Message'])
    
    slack_message = {
        "text": f"ğŸš¨ *{msg['AlarmName']}*",
        "attachments": [{
            "color": "danger" if msg['NewStateValue'] == "ALARM" else "good",
            "fields": [
                {"title": "Description", "value": msg['AlarmDescription']},
                {"title": "Reason", "value": msg['NewStateReason']},
                {"title": "Time", "value": msg['StateChangeTime']}
            ]
        }]
    }
    
    http.request('POST', url, 
                body=json.dumps(slack_message),
                headers={'Content-Type': 'application/json'})
```

## ì•ŒëŒ ì„ê³„ê°’ ì¡°ì •

### í™˜ê²½ë³„ ì„ê³„ê°’ ì„¤ì •

```hcl
# Dev í™˜ê²½ (ê´€ëŒ€í•œ ì„ê³„ê°’)
error_rate_threshold   = 10    # 10%
latency_p95_threshold  = 2000  # 2ì´ˆ
availability_threshold = 99    # 99%

# Production í™˜ê²½ (ì—„ê²©í•œ ì„ê³„ê°’)
error_rate_threshold   = 1     # 1%
latency_p95_threshold  = 500   # 500ms
availability_threshold = 99.95 # 99.95%
```

## ì•ŒëŒ ìš°ì„ ìˆœìœ„

### Critical (ì¦‰ì‹œ ëŒ€ì‘)
- **Low Availability**: ì„œë¹„ìŠ¤ ê°€ìš©ì„± SLO ìœ„ë°˜
- **OTEL Collector Down**: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë‹¤ìš´
- **Service Degradation**: ë³µí•© ì¥ì•  ìƒí™©

### High (30ë¶„ ë‚´ ëŒ€ì‘)
- **High Error Rate**: ì—ëŸ¬ìœ¨ ê¸‰ì¦
- **Pod Restart Rate High**: ì•ˆì •ì„± ë¬¸ì œ

### Medium (ì—…ë¬´ì‹œê°„ ë‚´ ëŒ€ì‘)
- **High Latency P95**: ì„±ëŠ¥ ì €í•˜
- **Node CPU/Memory High**: ë¦¬ì†ŒìŠ¤ ë¶€ì¡±

## ì•ŒëŒ ë°œìƒ ì‹œ ëŒ€ì‘

### 1. Low Availability ì•ŒëŒ

```bash
# 1. Pod ìƒíƒœ í™•ì¸
kubectl get pods -n production --field-selector status.phase!=Running

# 2. ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ í™•ì¸
kubectl logs -n production deployment/api --tail=100 | grep ERROR

# 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘ (í•„ìš”ì‹œ)
kubectl rollout restart deployment/api -n production
```

### 2. High Error Rate ì•ŒëŒ

```bash
# 1. ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
aws logs insights query \
  --log-group-name /aws/eks/nebula-eks-prod/application \
  --query 'fields @timestamp, @message | filter @message like /ERROR/'

# 2. X-Ray íŠ¸ë ˆì´ìŠ¤ í™•ì¸
aws xray get-trace-summaries --time-range-type LastHour
```

### 3. OTEL Collector Down ì•ŒëŒ

```bash
# 1. Collector Pod ìƒíƒœ í™•ì¸
kubectl get pods -n monitoring -l app=otel-collector

# 2. Collector ë¡œê·¸ í™•ì¸
kubectl logs -n monitoring deployment/otel-collector --tail=50

# 3. Collector ì¬ì‹œì‘
kubectl rollout restart deployment/otel-collector -n monitoring
```

## ì•ŒëŒ ëŒ€ì‹œë³´ë“œ

Grafanaì—ì„œ ì•ŒëŒ ìƒíƒœ ëª¨ë‹ˆí„°ë§:

```promql
# ì•ŒëŒ ìƒíƒœ ì¿¼ë¦¬
ALERTS{alertstate="firing"}

# ì•ŒëŒ íˆìŠ¤í† ë¦¬
increase(cloudwatch_alarm_state_changes_total[24h])
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì•ŒëŒì´ ë°œìƒí•˜ì§€ ì•Šì„ ë•Œ

1. **ë©”íŠ¸ë¦­ í™•ì¸**
```bash
aws cloudwatch get-metric-statistics \
  --namespace "Nebula/Application" \
  --metric-name "Errors" \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-01T01:00:00Z \
  --period 300 \
  --statistics Sum
```

2. **ì•ŒëŒ ìƒíƒœ í™•ì¸**
```bash
aws cloudwatch describe-alarms \
  --alarm-names "prod-high-error-rate"
```

3. **SNS êµ¬ë… í™•ì¸**
```bash
aws sns list-subscriptions-by-topic \
  --topic-arn arn:aws:sns:region:account:topic-name
```

### ë„ˆë¬´ ë§ì€ ì•ŒëŒì´ ë°œìƒí•  ë•Œ

1. **ì„ê³„ê°’ ì¡°ì •**
2. **Evaluation Periods ì¦ê°€**
3. **Composite Alarm í™œìš©**

## ì°¸ê³  ìë£Œ

- [CloudWatch Alarms ë¬¸ì„œ](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
- [Composite Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html)
- [SNS ì„¤ì • ê°€ì´ë“œ](https://docs.aws.amazon.com/sns/latest/dg/welcome.html)
